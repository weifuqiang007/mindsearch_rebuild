# # from dotenv import load_dotenv
# # import os
# # import sys
# #
# # print(os.path.dirname(os.path.abspath(__file__)))
# # sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
# # from datetime import datetime
# # from typing import List, Dict, Any
# #
# # # é¿å…å¯¼å…¥coreæ¨¡å—ï¼Œå› ä¸ºå®ƒä¼šè§¦å‘ç›¸å¯¹å¯¼å…¥é—®é¢˜
# # # å½“ç›´æ¥è¿è¡Œè„šæœ¬æ—¶ï¼ŒPythonå°†å…¶è§†ä¸ºä¸»æ¨¡å—ï¼Œç›¸å¯¹å¯¼å…¥ä¼šå¤±è´¥
# # try:
# #     # å°è¯•ç›´æ¥å¯¼å…¥search_toolsæ¨¡å—ï¼Œé¿å…é€šè¿‡core.__init__.py
# #     import core.search_tools as search_tools
# #     import config
# #
# #     SearchToolManager = search_tools.SearchToolManager
# #     SearchEngine = search_tools.SearchEngine
# #     SearchResult = search_tools.SearchResult
# #     get_search_manager = search_tools.get_search_manager
# #     get_settings = config.get_settings
# #
# #     print("âœ… æˆåŠŸå¯¼å…¥æœç´¢å·¥å…·")
# # except ImportError as e:
# #     print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
# #     print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
# #     print("1. ä½¿ç”¨ 'python -m langchain_rebuild.test' è¿è¡Œï¼ˆä½œä¸ºæ¨¡å—ï¼‰")
# #     print("2. æˆ–è€…å°†æ­¤æ–‡ä»¶ç§»åˆ°é¡¹ç›®æ ¹ç›®å½•å¤–")
# #     print("3. æˆ–è€…ä¿®æ”¹å¯¼å…¥æ–¹å¼é¿å…ç›¸å¯¹å¯¼å…¥é—®é¢˜")
# #     sys.exit(1)
# #
# # class TraditionalSearchExample:
# #     def __init__(self):
# #         pass
# #
# #     def search_with_requests(self, query: str) -> Dict[str, Any]:
# #         pass
# #
# # class SearchEngineTest:
# #     def __init__(self):
# #         self.settings = get_settings()
# #         self.search_manager = get_search_manager()
# #         self.traditional_search = TraditionalSearchExample()
# #
# #     def print_config_info(self):
# #         print("=" * 60)
# #         print("æœç´¢å¼•æ“é…ç½®ä¿¡æ¯")
# #         print("=" * 60)
# #
# #         search_config = self.settings.search
# #         print(f"é»˜è®¤æœç´¢å¼•æ“: {search_config.default_engine}")
# #         print(f"Google API Key: {'å·²é…ç½®' if search_config.google_api_key else 'æœªé…ç½®'}")
# #         print(f"Google æœç´¢å¼•æ“ID: {'å·²é…ç½®' if search_config.google_search_engine_id else 'æœªé…ç½®'}")
# #         print(f"Bing API Key: {'å·²é…ç½®' if search_config.bing_api_key else 'æœªé…ç½®'}")
# #         print(f"Serper API Key: {'å·²é…ç½®' if search_config.serper_api_key else 'æœªé…ç½®'}")
# #
# #         available_engines = self.search_manager.list_engines()
# #         print(f"å¯ç”¨æœç´¢å¼•æ“: {[engine.value for engine in available_engines]}")
# #         print()
# #         pass
# #
# # def main():
# #     """ä¸»æµ‹è¯•å‡½æ•°"""
# #     print("ğŸš€ æœç´¢å¼•æ“æµ‹è¯•å¼€å§‹")
# #     print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
# #
# #     # åˆå§‹åŒ–æµ‹è¯•
# #     test = SearchEngineTest()
# #
# #     test.print_config_info()
#
# # from dotenv import load_dotenv, dotenv_values
# #
# # env_file = ".dev_env"
# # env_path = ".dev_env"
# #
# # dev_env_vars = dotenv_values(env_path)
# # print(dev_env_vars)
# from pydantic_settings import BaseSettings, SettingsConfigDict
# need_load_dev_env = ".dev_env"
# from pydantic import Field
# from config import LLMConfig, SearchConfig, ServerConfig, PostgreSQLConfig, RedisConfig, AgentConfig
#
#
# class Settings(BaseSettings):
#     """ä¸»é…ç½®ç±»"""
#
#     model_config = SettingsConfigDict(
#         env_file=need_load_dev_env,
#         env_file_encoding="utf-8",
#         case_sensitive=False,
#         extra="ignore"
#     )
#
#     # å­é…ç½®
#     llm: LLMConfig = Field(default_factory=LLMConfig)
#     search: SearchConfig = Field(default_factory=SearchConfig)
#     server: ServerConfig = Field(default_factory=ServerConfig)
#     postgres: PostgreSQLConfig = Field(default_factory=PostgreSQLConfig)
#     redis: RedisConfig = Field(default_factory=RedisConfig)
#     agent: AgentConfig = Field(default_factory=AgentConfig)
#
#     # å…¨å±€é…ç½®
#     app_name: str = Field(default="MindSearch LangChain", env="APP_NAME")
#     version: str = Field(default="0.1.0", env="VERSION")
#     environment: str = Field(default="development", env="ENVIRONMENT")
#
#     def __init__(self, **kwargs):
#         # æå–_env_fileå‚æ•°
#         env_file = kwargs.get('_env_file', '.env')
#
#         # ä¸ºæ‰€æœ‰å­é…ç½®ä¼ é€’_env_fileå‚æ•°
#         if 'llm' not in kwargs:
#             kwargs['llm'] = LLMConfig(_env_file=env_file)
#         if 'search' not in kwargs:
#             kwargs['search'] = SearchConfig(_env_file=env_file)
#         if 'server' not in kwargs:
#             kwargs['server'] = ServerConfig(_env_file=env_file)
#         if 'postgres' not in kwargs:
#             kwargs['postgres'] = PostgreSQLConfig(_env_file=env_file)
#         if 'redis' not in kwargs:
#             kwargs['redis'] = RedisConfig(_env_file=env_file)
#         if 'agent' not in kwargs:
#             kwargs['agent'] = AgentConfig(_env_file=env_file)
#
#         super().__init__(**kwargs)
#
#
# if __name__ == "__main__":
#     settings = Settings(_env_file=".dev_env")
#     print(settings.llm)
# å®šä¹‰llmçš„ç®¡ç†å™¨
# from abc import ABC, abstractmethod
# from enum import Enum
# from typing import Optional, Dict, Any, List, AsyncGenerator
#
# import asyncio
#
# # è¿™ä¸ªåŒ…æ˜¯å¹²ä»€ä¹ˆç”¨çš„
# from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
# # from langchain_community.chat_models import ChatOpenAI as CommunityChatOpenAI
# from langchain_openai import ChatOpenAI
#
#
# try:
#     from ..config import get_settings
# except ImportError:
#     # å½“ç›´æ¥è¿è¡Œè„šæœ¬æ—¶ï¼Œç›¸å¯¹å¯¼å…¥ä¼šå¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
#     from langchain_rebuild.config import get_settings
#
#
# class LLMProvider(Enum):
#     """LLM æä¾›å•†æšä¸¾"""
#     OPENAI = "openai"
#     ANTHROPIC = "anthropic"
#     SILICONFLOW = "siliconflow"
#     LOCAL = "local"
#
# class BaseLLMWrapper(ABC):
#     """LLMåŒ…è£…å™¨åŸºç±»"""
#
#     def __init__(self, provider: LLMProvider, **kwargs):
#         self.provider = provider
#         self.config = kwargs
#         self._llm = None
#         self._initialize()
#
#     @abstractmethod
#     def _initialize(self):
#         """åˆå§‹åŒ–LLMå®ä¾‹"""
#         pass
#
#     @abstractmethod
#     async def agenerate(self, messages: List[BaseMessage], **kwargs) -> str:
#         """å¼‚æ­¥ç”Ÿæˆå“åº”"""
#         pass
#
#     @abstractmethod
#     async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
#         """å¼‚æ­¥æµå¼ç”Ÿæˆå“åº”"""
#         pass
#
#     def generate(self, messages: List[BaseMessage], **kwargs) -> str:
#         """åŒæ­¥ç”Ÿæˆå“åº”"""
#         return asyncio.run(self.agenerate(messages, **kwargs))
#
#
# class SiliconFlowWrapper(BaseLLMWrapper):
#     """SiliconFlow LLM åŒ…è£…å™¨"""
#
#     def _initialize(self):
#         settings = get_settings()
#         self._llm = ChatOpenAI(
#             api_key=settings.llm.siliconflow_api_key,
#             base_url=settings.llm.siliconflow_base_url,
#             model=settings.llm.siliconflow_model,
#             temperature=settings.llm.temperature,
#             max_tokens=settings.llm.max_tokens,
#             timeout=settings.llm.timeout,
#             streaming=True,
#             **self.config
#         )
#
#     async def agenerate(self, messages: List[BaseMessage], **kwargs) -> str:
#         """å¼‚æ­¥ç”Ÿæˆå“åº”"""
#         response = await self._llm.agenerate(messages, **kwargs)
#         return response.generations[0][0].text
#
#     async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
#         """å¼‚æ­¥æµå¼ç”Ÿæˆå“åº”"""
#         async for chunk in self._llm.astream(messages, **kwargs):
#             if chunk.content:
#                 yield chunk.content
#
#
# llm = SiliconFlowWrapper(provider=LLMProvider.SILICONFLOW)
# messages = [HumanMessage(content="Hello, how are you?")]
# # messages = [BaseMessage(content="Hello, how are you?", type="human")]
# print(asyncio.run(llm.agenerate(messages)))  # åŒæ­¥è°ƒç”¨æµ‹è¯•


from abc import ABC, abstractmethod
from enum import Enum
from typing import List, AsyncGenerator
import asyncio
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage


try:
    from ..config import get_settings
except ImportError:
    # å½“ç›´æ¥è¿è¡Œè„šæœ¬æ—¶ï¼Œç›¸å¯¹å¯¼å…¥ä¼šå¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    from langchain_rebuild.config import get_settings


class LLMProvider(Enum):
    """LLM æä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    SILICONFLOW = "siliconflow"
    LOCAL = "local"

class BaseLLMWrapper(ABC):
    """LLMåŒ…è£…å™¨åŸºç±»"""

    def __init__(self, provider: LLMProvider, **kwargs):
        self.provider = provider
        self.config = kwargs
        self._llm = None
        self._initialize()

    @abstractmethod
    def _initialize(self):
        """åˆå§‹åŒ–LLMå®ä¾‹"""
        pass

    @abstractmethod
    async def agenerate(self, messages: List[BaseMessage], **kwargs) -> str:
        """å¼‚æ­¥ç”Ÿæˆå“åº”"""
        pass

    @abstractmethod
    async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
        """å¼‚æ­¥æµå¼ç”Ÿæˆå“åº”"""
        pass

    def generate(self, messages: List[BaseMessage], **kwargs) -> str:
        """åŒæ­¥ç”Ÿæˆå“åº”"""
        return asyncio.run(self.agenerate(messages, **kwargs))


class SiliconFlowWrapper(BaseLLMWrapper):
    def _initialize(self):
        settings = get_settings()
        self._llm = ChatOpenAI(
            api_key=settings.llm.siliconflow_api_key,
            base_url=settings.llm.siliconflow_base_url,
            model=settings.llm.siliconflow_model,
            temperature=settings.llm.temperature,
            max_tokens=settings.llm.max_tokens,
            timeout=settings.llm.timeout,
            streaming=True,
            **self.config
        )

    # agenerate éœ€è¦æ¥æ”¶çš„æ˜¯ä¸€ä¸ª åˆ—è¡¨çš„åˆ—è¡¨ ([messages])ï¼Œå› ä¸º LangChain è®¾è®¡å¦‚æ­¤ï¼ˆæ”¯æŒæ‰¹é‡ç”Ÿæˆï¼‰ã€‚
    async def agenerate(self, messages: List[BaseMessage], **kwargs) -> str:
        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        if not all(isinstance(msg, (HumanMessage, AIMessage, SystemMessage)) for msg in messages):
            raise ValueError("Messages must be HumanMessage/AIMessage/SystemMessage")

        response = await self._llm.agenerate([messages], **kwargs)  # âœ… æ³¨æ„éœ€è¦ [messages]
        return response.generations[0][0].text

    async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
        async for chunk in self._llm.astream(messages, **kwargs):
            if chunk.content:
                yield chunk.content


# æµ‹è¯•
llm = SiliconFlowWrapper(provider=LLMProvider.SILICONFLOW)
messages = [HumanMessage(content="pythonä¸­å¤šçº¿ç¨‹å’Œåç¨‹æ˜¯ä»€ä¹ˆå‘€ï¼Ÿè¯­æ³•æ€ä¹ˆå†™ï¼Ÿæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿèƒ½ä¸èƒ½ç»™å†™ä¸€äº›ä»£ç ç¤ºä¾‹ï¼Ÿå°½é‡ç”¨é€šä¿—æ˜“æ‡‚å’Œä¸¾ä¾‹å­ã€åšå¯¹æ¯”çš„æ–¹å¼è¿›è¡Œè®²è§£ã€‚"), SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå¼€å‘çš„ä¸“å®¶ï¼Œå°¤å…¶æ˜¯åœ¨AIç¼–ç¨‹é¢†åŸŸæœ‰éå¸¸ä¸°å¯Œçš„ç»éªŒï¼Œä½ éœ€è¦éå¸¸ç»†è‡´çš„å°†æˆ‘çš„é—®é¢˜è¿›è¡Œå›ç­”ã€‚è€Œä¸”éœ€è¦é…åˆä¸Šä¸€äº›ä»£ç ç¤ºä¾‹è¿›è¡Œè®²è§£å›ç­”")]  # âœ… æ ‡å‡†æ¶ˆæ¯æ ¼å¼
# print(asyncio.run(llm.agenerate(messages)))
# print(asyncio.run(llm.astream(messages)))


async def run_stream():
    llm = SiliconFlowWrapper(provider=LLMProvider.SILICONFLOW)
    messages = [HumanMessage(content="pythonä¸­å¤šçº¿ç¨‹å’Œåç¨‹æ˜¯ä»€ä¹ˆå‘€ï¼Ÿè¯­æ³•æ€ä¹ˆå†™ï¼Ÿæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿèƒ½ä¸èƒ½ç»™å†™ä¸€äº›ä»£ç ç¤ºä¾‹ï¼Ÿå°½é‡ç”¨é€šä¿—æ˜“æ‡‚å’Œä¸¾ä¾‹å­ã€åšå¯¹æ¯”çš„æ–¹å¼è¿›è¡Œè®²è§£ã€‚"),
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå¼€å‘çš„ä¸“å®¶ï¼Œå°¤å…¶æ˜¯åœ¨AIç¼–ç¨‹é¢†åŸŸæœ‰éå¸¸ä¸°å¯Œçš„ç»éªŒï¼Œä½ éœ€è¦éå¸¸ç»†è‡´çš„å°†æˆ‘çš„é—®é¢˜è¿›è¡Œå›ç­”ã€‚è€Œä¸”éœ€è¦é…åˆä¸Šä¸€äº›ä»£ç ç¤ºä¾‹è¿›è¡Œè®²è§£å›ç­”")]  # âœ… æ ‡å‡†æ¶ˆæ¯æ ¼å¼
    # astream() æœ¬èº«æ˜¯å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œå¿…é¡»ç”¨ async for æ¶ˆè´¹ã€‚
    # ä»»ä½•åŒ…å« async for æˆ– await çš„ä»£ç å—å¿…é¡»ä½äº async def å‡½æ•°å†…ï¼ˆPythonçš„å¼ºåˆ¶è¦æ±‚ï¼‰ã€‚
    async for chunk in llm.astream(messages):
        print(chunk, end="", flush=True) # å®æ—¶æ‰“å°æµå¼ç»“æœ

asyncio.run(run_stream())


